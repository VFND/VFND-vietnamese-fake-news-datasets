{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dùng get-news để crawl dữ liệu tin tức"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Change log\n",
    "Ver0.1: Sử dụng thư viện [news-please](https://github.com/fhamborg/news-please), nhược điểm chưa lấy được URL của hình ảnh và videos\n",
    "\n",
    "Ver0.2: Sử dụng [fake-useragent](https://pypi.org/project/fake-useragent/) để tạo _rotating proxy_ để có thể hỗ trợ lấy tin\n",
    "\n",
    "Ver0.3: Sử dụng thêm thư viện BeautifulSoup để lấy thêm được URL của ảnh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Thư viện và các function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "from newsplease import NewsPlease\n",
    "# from bs4 import BeautifulSoup\n",
    "# from fake_useragent import UserAgent\n",
    "\n",
    "# from urllib.request import Request, urlopen\n",
    "\n",
    "# ua = UserAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Các function liên quan đến tin tức\n",
    "Từ các url tin tức, chúng ta đưa chúng vào trong 1 list, hàm này dùng thư viên _NewsPlease_ để đưa dữ liệu tin tức lấy được vào 1 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_to_list(url_list):\n",
    "    news_list = []\n",
    "    for url in url_list:\n",
    "        try:\n",
    "            news_list.append(NewsPlease.from_url(url))\n",
    "        except Exception as e: \n",
    "            print(e, ': ', url)\n",
    "            continue\n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đưa các dữ liệu sẵn có vào file JSON với cấu trúc là ```<json_prefix>_<json_no>.json```\n",
    "\n",
    "Các thông số cho json.dump lần lượt là: __default = str__ đưa tất cả về string để tránh lỗi trong quá trình parse dữ liệu dạng _datetime_ sang json của các trường date\\_modify, date\\_download, date\\_publish; __ensure\\_ascii=False__ là để giữ các dữ liệu text ở dạng Unicode - để không bị lỗi khi đưa vào file json khi để text ở định dạng Unicode, thì trong phần __open()__ đặt thông số _encoding='utf-8'_. \n",
    "\n",
    "Các biến:\n",
    "1. file\\_name: tên file để lưu\n",
    "2. file\\_no: số thứ tự của file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_news_to_json(news_list, json_begin_no, json_prefix, file_path):\n",
    "    file_no = json_begin_no\n",
    "    for news in news_list:\n",
    "        file_name = json_prefix + str(file_no) + '.json' \n",
    "        file_no += 1\n",
    "        with open(os.path.join(file_path, file_name), 'w', encoding='utf-8') as outfile:\n",
    "            json.dump(news.__dict__, outfile, indent=4, sort_keys=True, default=str, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Các function liên quan đến việc tạo rotating proxy\n",
    "Tham khảo tại một số trang sau:\n",
    "1. [Create a rotating proxy crawler in Python 3](https://codelike.pro/create-a-crawler-with-rotating-ip-proxy-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_proxy(proxies):\n",
    "  return random.randint(0, len(proxies) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrive_proxies():\n",
    "    proxies_req = Request('https://sslproxies.org/')\n",
    "    proxies_req.add_header('User-Agent', ua.random)\n",
    "    proxies_doc = urlopen(proxies_req).read().decode('utf8')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Các dữ liệu tin tức giả\n",
    "URL về Fake News, tập hợp từ các bài viết có nội dung không được xác minh về nguồn tin, hoặc được xác nhận sai,... Có thể lưu url các bài báo trong 1 file text (mỗi url là 1 dòng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_urls = [\n",
    "    \"https://saostar.vn/the-gioi/ten-lua-phong-khong-ukraine-tu-ban-trung-toa-chung-cu-o-kiev-202202271128525058.html\",\n",
    "    \"https://infonet.vietnamnet.vn/the-gioi/ukraine-ten-lua-ban-vao-nha-dan-khong-ben-nao-nhan-trach-nhiem-168057.html\",\n",
    "    \"https://tuoitre.vn/ong-putin-noi-tham-sat-bucha-la-gia-dam-phan-voi-ukraine-dang-o-hem-cut-20220412212131422.htm\",\n",
    "    \"https://baomoi.com/nga-noi-phuong-tay-giup-ukraine-nguy-tao-cao-buoc-tham-sat/c/42284586.epi\",\n",
    "    \n",
    "]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bắt đầu crawl dữ liệu tin tức giả đưa vào list fake_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_articles = get_news_to_list(fake_news_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tên của các file json sẽ chứa nội dung vừa crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_no = 52\n",
    "fake_prefix = 'VFND_Ac_Pro_Russia_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đưa dữ liệu đã lấy vào các file JSON để lưu trữ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_news_to_json(fake_articles, fake_no, fake_prefix, './')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test các kết quả trả ra khi crawl ở cell dưới"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "text1 = \"\"\n",
    "\n",
    "for arc in fake_articles:\n",
    "    num = fake_no + i\n",
    "    text1 = text1 + fake_prefix + str(num) + \": [\" + arc.title + \"](\" + arc.url + \")\\n\\n\"\n",
    "\n",
    "    i = i+1\n",
    "\n",
    "print(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Các dữ liệu tin tức thật"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news_urls = [\n",
    "    \"https://nld.com.vn/thoi-su-quoc-te/ukraine-phat-hien-may-chu-co-kha-nang-ven-man-vu-tham-sat-bucha-20220412131157701.htm\",\n",
    "    \"https://trithucvn.org/the-gioi/tham-sat-o-bucha-ukraine-phat-hien-phong-tra-tan-cua-nga-trong-khu-vuc.html\"\n",
    "\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bắt đầu crawl dữ liệu, sử dụng thư viện news-please, các tin tức sẽ được lấy theo URL và được chứa trong mảng _real_\\__article_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_articles = get_news_to_list(real_news_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do thư viện _news-please_ không hỗ trợ để lấy hình ảnh vì thế chúng ta sẽ lấy hình ảnh thông qua thư viện _BeautifulSoup_. Sau mỗi lần lấy chúng ta tạo ra 1 đối tượng dictionary cho mỗi URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_imgs = []\n",
    "for real_url in real_news_urls:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge 2 đối tượng dictionary. Tham khảo từ: [How to merge two dictionaries in a single expression? - StackOverflow](https://stackoverflow.com/questions/38987/how-to-merge-two-dictionaries-in-a-single-expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tương tự với việc crawl dữ liệu về tin tức giả. Ta có danh sách 1 số biến:\n",
    "1. real\\_name: tên file để lưu\n",
    "2. real\\_no: số thứ tự của file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_no = 6\n",
    "real_prefix = 'VFND_Ac_Ukrainian_POV_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for real in real_articles:\n",
    "    real_name = real_prefix + str(real_no) + '.json'\n",
    "    real_no = real_no + 1\n",
    "    with open(real_name, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(real.__dict__, outfile, indent=4, sort_keys=True, default=str, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test các kết quả trả ra khi crawl ở cell dưới"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "text2 = \"\"\n",
    "\n",
    "for arc in real_articles:\n",
    "    num = real_no + j\n",
    "    text2 = text2 + real_prefix + str(num) + \": [\" + arc.title + \"](\" + arc.url + \")\\n\\n\"\n",
    "\n",
    "    j = j+1\n",
    "\n",
    "print(text2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
