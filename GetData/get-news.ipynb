{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dùng get-news để crawl dữ liệu tin tức"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Change log\n",
    "Ver0.1: Sử dụng thư viện [news-please](https://github.com/fhamborg/news-please), nhược điểm chưa lấy được URL của hình ảnh và videos\n",
    "\n",
    "Ver0.2: Sử dụng thêm thư viện BeautifulSoup để lấy thêm được URL của ảnh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import thư viện để crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from newsplease import NewsPlease\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Các dữ liệu tin tức giả\n",
    "URL về Fake News, tập hợp từ các bài viết có nội dung không được xác minh về nguồn tin, hoặc được xác nhận sai,... Có thể lưu url các bài báo trong 1 file text (mỗi url là 1 dòng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_urls = [\"\", \n",
    "                 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bắt đầu crawl dữ liệu tin tức giả đưa vào list fake_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_articles = []\n",
    "for fake_url in fake_news_urls:\n",
    "    fake_articles.append(NewsPlease.from_url(fake_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tên của các file json sẽ chứa nội dung vừa crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_no = 1\n",
    "fake_prefix = 'VFND_Ac_Fake_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đưa các dữ liệu vừa crawl vào các file json dữ liệu. Các thông số cho json.dump lần lượt là: __default = str__ đưa tất cả về string để tránh lỗi trong quá trình parse dữ liệu dạng _datetime_ sang json của các trường date\\_modify, date\\_download, date\\_publish; __ensure\\_ascii=False__ là để giữ các dữ liệu text ở dạng Unicode - để không bị lỗi khi đưa vào file json khi để text ở định dạng Unicode, thì trong phần __open()__ đặt thông số _encoding='utf-8'_. \n",
    "\n",
    "Các biến:\n",
    "1. fake\\_name: tên file để lưu\n",
    "2. fake\\_no: số thứ tự của file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fake in fake_articles:\n",
    "    fake_name = fake_prefix + str(fake_no) + '.json' \n",
    "    fake_no = fake_no + 1\n",
    "    \n",
    "    with open(fake_name, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(fake.__dict__, outfile, indent=4, sort_keys=True, default=str, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test các kết quả trả ra khi crawl ở cell dưới"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'authors': [], 'date_download': datetime.datetime(2018, 9, 25, 23, 13, 9), 'date_modify': None, 'date_publish': datetime.datetime(2018, 6, 29, 15, 25, 8), 'description': None, 'filename': 'http%3A%2F%2Fbinhluan.biz%2Fthu-tuong-abe-cui-dau-xin-loi-vi-hanh-dong-phi-the-thao-cua-tuyen-nhat.html.json', 'image_url': 'http://static.binhluan.biz/bongda/images/favicon.ico', 'language': 'en', 'localpath': None, 'title': 'Thủ tướng Abe cúi đầu xin lỗi vì hành động phi thể thao của tuyển Nhật', 'title_page': None, 'title_rss': None, 'source_domain': 'binhluan.biz', 'text': None, 'url': 'http://binhluan.biz/thu-tuong-abe-cui-dau-xin-loi-vi-hanh-dong-phi-the-thao-cua-tuyen-nhat.html'}\n"
     ]
    }
   ],
   "source": [
    "print (fake_articles[0].__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Các dữ liệu tin tức thật"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news_urls = [\n",
    "                    ''\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bắt đầu crawl dữ liệu, sử dụng thư viện news-please, các tin tức sẽ được lấy theo URL và được chứa trong mảng _real_\\__article_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_articles = []\n",
    "for real_url in real_news_urls:\n",
    "    real_articles.append(NewsPlease.from_url(real_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do thư viện _news-please_ không hỗ trợ để lấy hình ảnh vì thế chúng ta sẽ lấy hình ảnh thông qua thư viện _BeautifulSoup_. Sau mỗi lần lấy chúng ta tạo ra 1 đối tượng dictionary cho mỗi URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-4-ae902ad66bab>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-ae902ad66bab>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "real_imgs = []\n",
    "for real_url in real_news_urls:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge 2 đối tượng dictionary. Tham khảo từ: [How to merge two dictionaries in a single expression? - StackOverflow](https://stackoverflow.com/questions/38987/how-to-merge-two-dictionaries-in-a-single-expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tương tự với việc crawl dữ liệu về tin tức giả. Ta có danh sách 1 số biến:\n",
    "1. real\\_name: tên file để lưu\n",
    "2. real\\_no: số thứ tự của file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_no = 9\n",
    "real_prefix = 'VFND_Ac_Real_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for real in real_articles:\n",
    "    real_name = real_prefix + str(real_no) + '.json'\n",
    "    real_no = real_no + 1\n",
    "    with open(real_name, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(real.__dict__, outfile, indent=4, sort_keys=True, default=str, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test các kết quả trả ra khi crawl ở cell dưới"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(real_articles[0].__dict__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
